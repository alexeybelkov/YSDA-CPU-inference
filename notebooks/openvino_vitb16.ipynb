{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT: 4 4\n",
      "4 4\n",
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 4\n",
      "\tat::get_num_interop_threads() : 4\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 4\n",
      "Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 4\n",
      "Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
      "std::thread::hardware_concurrency() : 8\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch.multiprocessing\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "print('DEFAULT:', torch.get_num_threads(), torch.get_num_interop_threads())\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# os.environ['MKL_NUM_THREADS'] = '1'\n",
    "# torch.set_num_threads(1), torch.set_num_interop_threads(1)\n",
    "print(torch.get_num_threads(), torch.get_num_interop_threads())\n",
    "print(torch.__config__.parallel_info())\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "transforms = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../imagenet1000.txt', 'r') as fopen:\n",
    "    lines = fopen.readlines()\n",
    "\n",
    "def process_classes(line: str):\n",
    "    splitted = line.strip().removeprefix('{').removesuffix(',').split(':')\n",
    "    return (int(splitted[0]), splitted[1].strip().strip('\\''))\n",
    "\n",
    "orig_classes = dict(map(process_classes, lines))\n",
    "\n",
    "imagenette_classes = dict(enumerate(['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']))\n",
    "\n",
    "for k, v in imagenette_classes.items():\n",
    "    for k1, v1 in orig_classes.items():\n",
    "        if v in v1:\n",
    "            imagenette_classes[k] = k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasource, transforms: callable):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.datasource = datasource\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datasource)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        data = self.datasource[index]\n",
    "        image, label = data['image'], data['label']\n",
    "        if image.mode != 'RGB':\n",
    "            image = Image.fromarray(np.array(image)[..., None].repeat(3, -1))\n",
    "        return self.transforms(image), imagenette_classes[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_train = load_dataset('frgfm/imagenette', '320px', split='train')\n",
    "imagenette_valid = load_dataset('frgfm/imagenette', '320px', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms()\n",
    "trainset = Dataset(datasource=imagenette_train, transforms=tf)\n",
    "validset = Dataset(datasource=imagenette_valid, transforms=tf)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbytes(model: torch.nn.Module):\n",
    "    n = 0\n",
    "    for p in model.parameters():\n",
    "        n += p.nbytes\n",
    "    return n / 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f730ccbff70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from itertools import product\n",
    "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
    "from torch.ao.quantization import get_default_qconfig_mapping, get_default_qconfig\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "from timeit import timeit\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "import datetime\n",
    "# import torch.quantization._numeric_suite as ns\n",
    "import torch.quantization._numeric_suite_fx as ns\n",
    "\n",
    "def fix_seed(worker_id=0, seed=0xBADCAFE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "torch_generator = torch.Generator()\n",
    "torch_generator.manual_seed(0xBADCAFFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_model = deepcopy(model)\n",
    "module_a = deepcopy(model)\n",
    "module_a.heads.head = nn.Identity()\n",
    "module_b = model.heads.head\n",
    "# model = LoggerModule(module_a, module_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:nncf:NNCF provides best results with torch==2.0.1, while current torch version is 2.1.0+cu121. If you encounter issues, consider switching to torch==2.0.1\n",
      "INFO:nncf:No match has been found among the model operations for the following ignored/target scope definitions:\n",
      " - ignored_scope: ['{re}.*__truediv__*', '{re}.*matmul_1', '{re}.*Embeddings.*']\n",
      "Refer to the original_graph.dot to discover the operations in the model currently visible to NNCF and specify the ignored/target scopes in terms of the names there.\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 5 VisionTransformer/Encoder[encoder]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 7 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 9 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 10 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 16 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 17 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 19 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 20 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 26 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 27 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 29 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 30 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 36 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 37 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 39 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 40 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 46 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 47 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 49 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 50 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 56 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 57 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 59 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 60 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 66 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 67 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 69 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 70 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 76 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 77 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 79 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 80 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 86 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 87 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 89 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 90 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 96 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 97 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 99 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 100 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 106 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 107 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 109 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 110 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 116 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 117 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/NNCFLayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 119 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 120 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/NNCFLayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 126 VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 127 VisionTransformer/Encoder[encoder]/NNCFLayerNorm[ln]/layer_norm_0\n",
      "INFO:nncf:Collecting tensor statistics |█               | 113 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |███             | 226 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |█████           | 339 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |███████         | 452 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |████████        | 565 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |██████████      | 678 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |████████████    | 791 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |██████████████  | 904 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |███████████████ | 1017 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |████████████████| 1024 / 1024\n",
      "INFO:nncf:Compiling and loading torch extension: quantized_functions_cpu...\n",
      "INFO:nncf:Finished loading torch extension: quantized_functions_cpu\n"
     ]
    }
   ],
   "source": [
    "import nncf\n",
    "import torch\n",
    "\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    return images\n",
    "\n",
    "with torch.inference_mode():\n",
    "    calibration_dataset_a = nncf.Dataset(valid_dataloader, transform_fn)\n",
    "    quantized_module_a = nncf.quantize(module_a, calibration_dataset_a, model_type=nncf.ModelType.TRANSFORMER,  subset_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9280e544f7841c98dcc4edd18fa4d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Iterable, Optional, Union, List, Tuple, Any\n",
    "\n",
    "with torch.inference_mode():\n",
    "    embeddigs = [quantized_module_a(x) for x, _ in tqdm(valid_dataloader)]\n",
    "\n",
    "\n",
    "class IterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, source: Union[List, Tuple], labels: Optional[Union[List, Tuple]] = None):\n",
    "        self.source = source\n",
    "        self.labels = labels\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.source)\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        return self.source[index], index if self.labels is None else self.labels[index]\n",
    "\n",
    "embeddigs_dataset = IterDataset(embeddigs)\n",
    "embeddigs_loader = torch.utils.data.DataLoader(embeddigs_dataset, num_workers=num_workers, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Collecting tensor statistics |█               | 113 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |███             | 226 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |█████           | 339 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |███████         | 452 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |████████        | 565 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |██████████      | 678 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |████████████    | 791 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |██████████████  | 904 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |███████████████ | 1017 / 1024\n",
      "INFO:nncf:Collecting tensor statistics |████████████████| 1024 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |█               | 113 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |███             | 226 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |█████           | 339 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |███████         | 452 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████        | 565 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |██████████      | 678 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████    | 791 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |██████████████  | 904 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |███████████████ | 1017 / 1024\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████████| 1024 / 1024\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    calibration_dataset_b = nncf.Dataset(embeddigs_loader, transform_fn)\n",
    "    quantized_module_b = nncf.quantize(module_b, calibration_dataset_b, subset_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from openvino.tools.mo import convert_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/.local/lib/python3.10/site-packages/nncf/torch/quantization/layers.py:336: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self._level_low.item()\n",
      "/home/alexey/.local/lib/python3.10/site-packages/nncf/torch/quantization/layers.py:344: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self._level_high.item()\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/__init__.py:1404: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::_native_multi_head_attention' to ONNX opset version 17 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_f32 \u001b[39m=\u001b[39m validset[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(quantized_module_a, input_f32, \u001b[39m'\u001b[39;49m\u001b[39m../onnx/q_vitb16_a.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     input_f32 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand_like(embeddigs[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mexport(quantized_module_b, input_f32, \u001b[39m'\u001b[39m\u001b[39m../onnx/q_vitb16_b.onnx\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1597\u001b[0m     model,\n\u001b[1;32m   1598\u001b[0m     args,\n\u001b[1;32m   1599\u001b[0m     verbose,\n\u001b[1;32m   1600\u001b[0m     input_names,\n\u001b[1;32m   1601\u001b[0m     output_names,\n\u001b[1;32m   1602\u001b[0m     operator_export_type,\n\u001b[1;32m   1603\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1604\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1605\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1606\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1607\u001b[0m )\n\u001b[1;32m   1609\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1611\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1612\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1139\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     graph \u001b[39m=\u001b[39m _optimize_graph(\n\u001b[1;32m   1140\u001b[0m         graph,\n\u001b[1;32m   1141\u001b[0m         operator_export_type,\n\u001b[1;32m   1142\u001b[0m         _disable_torch_constant_prop\u001b[39m=\u001b[39;49m_disable_torch_constant_prop,\n\u001b[1;32m   1143\u001b[0m         fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1144\u001b[0m         params_dict\u001b[39m=\u001b[39;49mparams_dict,\n\u001b[1;32m   1145\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1146\u001b[0m         input_names\u001b[39m=\u001b[39;49minput_names,\n\u001b[1;32m   1147\u001b[0m         module\u001b[39m=\u001b[39;49mmodule,\n\u001b[1;32m   1148\u001b[0m     )\n\u001b[1;32m   1149\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1150\u001b[0m     torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mTorch IR graph at exception: \u001b[39m\u001b[39m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py:677\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    674\u001b[0m     _C\u001b[39m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    675\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 677\u001b[0m graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39;49m_jit_pass_onnx(graph, operator_export_type)\n\u001b[1;32m    678\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    679\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1950\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1946\u001b[0m     \u001b[39mif\u001b[39;00m namespace \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monnx\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1947\u001b[0m         \u001b[39m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m         \u001b[39mreturn\u001b[39;00m graph_context\u001b[39m.\u001b[39mop(op_name, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattrs, outputs\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39moutputsSize())  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1950\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1951\u001b[0m         symbolic_function_name,\n\u001b[1;32m   1952\u001b[0m         opset_version,\n\u001b[1;32m   1953\u001b[0m         symbolic_function_group\u001b[39m.\u001b[39mget_min_supported()\n\u001b[1;32m   1954\u001b[0m         \u001b[39mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1955\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1956\u001b[0m     )\n\u001b[1;32m   1958\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m   1959\u001b[0m     \u001b[39mif\u001b[39;00m operator_export_type \u001b[39m==\u001b[39m _C_onnx\u001b[39m.\u001b[39mOperatorExportTypes\u001b[39m.\u001b[39mONNX_FALLTHROUGH:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::_native_multi_head_attention' to ONNX opset version 17 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues."
     ]
    }
   ],
   "source": [
    "input_f32 = validset[0][0][None]\n",
    "with torch.inference_mode():\n",
    "    torch.onnx.export(quantized_module_a, input_f32, '../onnx/q_vitb16_a.onnx')\n",
    "    input_f32 = torch.rand_like(embeddigs[0])\n",
    "    torch.onnx.export(quantized_module_b, input_f32, '../onnx/q_vitb16_b.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception from src/bindings/python/src/pyopenvino/utils/utils.cpp:233:\nPath: 'VisionTransformer(\n  (conv_proj): NNCFConv2d(\n    3, 768, kernel_size=(16, 16), stride=(16, 16)\n    (pre_ops): ModuleDict(\n      (0): UpdateWeight(\n        (op): SymmetricQuantizer(bit=8, ch=True)\n      )\n    )\n    (post_ops): ModuleDict()\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): NNCFLayerNorm(\n      (768,), eps=1e-06, elementwise_affine=True\n      (pre_ops): ModuleDict()\n      (post_ops): ModuleDict()\n    )\n  )\n  (heads): Sequential(\n    (head): Identity()\n  )\n  (_nncf): NNCFNetworkInterface(\n    (external_quantizers): ModuleDict(\n      (/nncf_model_input_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n    )\n  )\n)' does not exist. Please provide valid model's path either as a string, bytes or pathlib.Path. Examples:\n(1) '/home/user/models/model.onnx'\n(2) Path('/home/user/models/model/model.onnx')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m int8_a \u001b[39m=\u001b[39m ov\u001b[39m.\u001b[39;49mcompile_model(quantized_module_a)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/openvino.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m int8_b \u001b[39m=\u001b[39m ov\u001b[39m.\u001b[39mcompile_model(quantized_module_b)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py:610\u001b[0m, in \u001b[0;36mcompile_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compact method to compile model with AUTO plugin.\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \n\u001b[1;32m    603\u001b[0m \u001b[39m:param model_path: Path to file with model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m \n\u001b[1;32m    608\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m core \u001b[39m=\u001b[39m Core()\n\u001b[0;32m--> 610\u001b[0m \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39;49mcompile_model(model_path, \u001b[39m\"\u001b[39;49m\u001b[39mAUTO\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py:543\u001b[0m, in \u001b[0;36mCore.compile_model\u001b[0;34m(self, model, device_name, config)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m device_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m CompiledModel(\n\u001b[1;32m    539\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcompile_model(model, {} \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m config),\n\u001b[1;32m    540\u001b[0m     )\n\u001b[1;32m    542\u001b[0m \u001b[39mreturn\u001b[39;00m CompiledModel(\n\u001b[0;32m--> 543\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcompile_model(model, device_name, {} \u001b[39mif\u001b[39;49;00m config \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m config),\n\u001b[1;32m    544\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/bindings/python/src/pyopenvino/utils/utils.cpp:233:\nPath: 'VisionTransformer(\n  (conv_proj): NNCFConv2d(\n    3, 768, kernel_size=(16, 16), stride=(16, 16)\n    (pre_ops): ModuleDict(\n      (0): UpdateWeight(\n        (op): SymmetricQuantizer(bit=8, ch=True)\n      )\n    )\n    (post_ops): ModuleDict()\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): NNCFLayerNorm(\n          (768,), eps=1e-06, elementwise_affine=True\n          (pre_ops): ModuleDict()\n          (post_ops): ModuleDict()\n        )\n        (mlp): MLPBlock(\n          (0): NNCFLinear(\n            in_features=768, out_features=3072, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): NNCFLinear(\n            in_features=3072, out_features=768, bias=True\n            (pre_ops): ModuleDict(\n              (0): UpdateWeight(\n                (op): SymmetricQuantizer(bit=8, ch=True)\n              )\n            )\n            (post_ops): ModuleDict()\n          )\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): NNCFLayerNorm(\n      (768,), eps=1e-06, elementwise_affine=True\n      (pre_ops): ModuleDict()\n      (post_ops): ModuleDict()\n    )\n  )\n  (heads): Sequential(\n    (head): Identity()\n  )\n  (_nncf): NNCFNetworkInterface(\n    (external_quantizers): ModuleDict(\n      (/nncf_model_input_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_0]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_10]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_11]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_1]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_2]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_3]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_4]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_5]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_6]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_7]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_8]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/MLPBlock[mlp]/GELU[1]/gelu_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n      (VisionTransformer/Encoder[encoder]/Sequential[layers]/EncoderBlock[encoder_layer_9]/NNCFLayerNorm[ln_2]/layer_norm_0|OUTPUT): AsymmetricQuantizer(bit=8, ch=False)\n    )\n  )\n)' does not exist. Please provide valid model's path either as a string, bytes or pathlib.Path. Examples:\n(1) '/home/user/models/model.onnx'\n(2) Path('/home/user/models/model/model.onnx')\n"
     ]
    }
   ],
   "source": [
    "int8_a = ov.compile_model(quantized_module_a)\n",
    "int8_b = ov.compile_model(quantized_module_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f833ba393bd41e9b744047f974ec3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "gt = []\n",
    "pred = []\n",
    "# embeddings = []\n",
    "Y = []\n",
    "with torch.inference_mode():\n",
    "    for x, y in tqdm(valid_dataloader):\n",
    "        emb = quantized_module_a(x)\n",
    "        # embeddings.append(emb)\n",
    "        y_hat = quantized_module_b(emb)\n",
    "        Y.append(y_hat)\n",
    "        gt.append(y)\n",
    "        pred.append(y_hat.argmax(-1))\n",
    "    gt = torch.cat(gt).ravel().numpy()\n",
    "    pred = torch.cat(pred).ravel().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330.2294006347656, 327.47565841674805)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbytes(model), nbytes(quantized_module_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8698089171974522"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20203"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
