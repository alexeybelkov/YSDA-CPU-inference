{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "          aten::linear         4.86%      88.000us       100.00%       1.811ms       1.811ms             1  \n",
      "               aten::t         2.15%      39.000us        26.50%     480.000us     480.000us             1  \n",
      "       aten::transpose        24.02%     435.000us        24.35%     441.000us     441.000us             1  \n",
      "      aten::as_strided         0.39%       7.000us         0.39%       7.000us       3.500us             2  \n",
      "           aten::addmm        48.81%     884.000us        68.64%       1.243ms       1.243ms             1  \n",
      "          aten::expand         1.05%      19.000us         1.10%      20.000us      20.000us             1  \n",
      "           aten::copy_        18.39%     333.000us        18.39%     333.000us     333.000us             1  \n",
      "    aten::resolve_conj         0.33%       6.000us         0.33%       6.000us       3.000us             2  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.811ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-11-15 17:32:58 39315:39315 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-15 17:32:58 39315:39315 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-15 17:32:58 39315:39315 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(256, 512)\n",
    "linear = nn.Linear(512, 256).eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        linear(x)\n",
    "print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::quantize_per_tensor        55.13%     543.000us        55.74%     549.000us     549.000us             1  \n",
      "                       aten::item         0.71%       7.000us         1.02%      10.000us       2.500us             4  \n",
      "        aten::_local_scalar_dense         0.30%       3.000us         0.30%       3.000us       0.750us             4  \n",
      "                quantized::linear        39.80%     392.000us        41.32%     407.000us     407.000us             1  \n",
      "    aten::_empty_affine_quantized         0.61%       6.000us         0.61%       6.000us       6.000us             1  \n",
      "                    aten::q_scale         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "               aten::q_zero_point         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                    aten::resize_         0.51%       5.000us         0.51%       5.000us       5.000us             1  \n",
      "                      aten::empty         0.61%       6.000us         0.61%       6.000us       3.000us             2  \n",
      "                 aten::dequantize         2.34%      23.000us         2.54%      25.000us      25.000us             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 985.000us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input : torch.Tensor) -> torch.Tensor:\n",
      "    input_1 = input\n",
      "    _input_scale_0 = self._input_scale_0\n",
      "    _input_zero_point_0 = self._input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(input_1, _input_scale_0, _input_zero_point_0, torch.quint8);  input_1 = _input_scale_0 = _input_zero_point_0 = None\n",
      "    _packed_weight_0 = self._packed_weight_0\n",
      "    _scale_1 = self._scale_1\n",
      "    _zero_point_1 = self._zero_point_1\n",
      "    linear = torch.ops.quantized.linear(quantize_per_tensor, _packed_weight_0, _scale_1, _zero_point_1);  quantize_per_tensor = _packed_weight_0 = _scale_1 = _zero_point_1 = None\n",
      "    dequantize_2 = linear.dequantize();  linear = None\n",
      "    return dequantize_2\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-11-15 17:41:34 39315:39315 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-15 17:41:34 39315:39315 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-15 17:41:34 39315:39315 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    qconfig = get_default_qconfig('x86')\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    example_inputs = torch.randn_like(x)\n",
    "    prepared_model = prepare_fx(linear, qconfig_mapping, example_inputs)\n",
    "    for _ in range(16):\n",
    "        prepared_model(torch.randn_like(x))\n",
    "\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        quantized_model(x)\n",
    "\n",
    "print(prof.key_averages().table())\n",
    "print(quantized_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::quantize_per_tensor        19.57%      92.000us        20.64%      97.000us      97.000us             1  \n",
      "                       aten::item         2.13%      10.000us         2.34%      11.000us       2.750us             4  \n",
      "        aten::_local_scalar_dense         0.21%       1.000us         0.21%       1.000us       0.250us             4  \n",
      "                quantized::linear        71.28%     335.000us        73.40%     345.000us     345.000us             1  \n",
      "    aten::_empty_affine_quantized         0.85%       4.000us         0.85%       4.000us       4.000us             1  \n",
      "                    aten::q_scale         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "               aten::q_zero_point         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                    aten::resize_         0.64%       3.000us         0.64%       3.000us       3.000us             1  \n",
      "                      aten::empty         1.06%       5.000us         1.06%       5.000us       2.500us             2  \n",
      "                 aten::dequantize         4.26%      20.000us         4.68%      22.000us      22.000us             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 470.000us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input : torch.Tensor) -> torch.Tensor:\n",
      "    input_1 = input\n",
      "    _input_scale_0 = self._input_scale_0\n",
      "    _input_zero_point_0 = self._input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(input_1, _input_scale_0, _input_zero_point_0, torch.quint8);  input_1 = _input_scale_0 = _input_zero_point_0 = None\n",
      "    _packed_weight_0 = self._packed_weight_0\n",
      "    _scale_1 = self._scale_1\n",
      "    _zero_point_1 = self._zero_point_1\n",
      "    linear = torch.ops.quantized.linear(quantize_per_tensor, _packed_weight_0, _scale_1, _zero_point_1);  quantize_per_tensor = _packed_weight_0 = _scale_1 = _zero_point_1 = None\n",
      "    dequantize_2 = linear.dequantize();  linear = None\n",
      "    return dequantize_2\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-11-15 17:41:44 39315:39315 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-15 17:41:44 39315:39315 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-15 17:41:44 39315:39315 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    qconfig = get_default_qconfig('fbgemm')\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    example_inputs = torch.randn_like(x)\n",
    "    prepared_model = prepare_fx(linear, qconfig_mapping, example_inputs)\n",
    "    for _ in range(16):\n",
    "        prepared_model(torch.randn_like(x))\n",
    "\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        quantized_model(x)\n",
    "\n",
    "print(prof.key_averages().table())\n",
    "print(quantized_model.code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
