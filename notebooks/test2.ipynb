{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(n, m)\n\u001b[1;32m      4\u001b[0m linear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(nn\u001b[38;5;241m.\u001b[39mLinear(m, n, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39meval(), nn\u001b[38;5;241m.\u001b[39mIdentity())\n\u001b[0;32m----> 5\u001b[0m \u001b[43mlinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# linear.bias.requires_grad = False\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "n = 2048\n",
    "m = 4096\n",
    "x = torch.rand(n, m)\n",
    "linear = nn.Sequential(nn.Linear(m, n, bias=False).eval(), nn.Identity())\n",
    "linear.weight.requires_grad = False\n",
    "# linear.bias.requires_grad = False\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        linear(x)\n",
    "print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jit = torch.jit.script(linear)\n",
    "torch.jit.save(jit, 'linear.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::quantize_per_tensor         2.43%       3.667ms         2.44%       3.676ms       3.676ms             1  \n",
      "                       aten::item         0.00%       7.000us         0.01%       9.000us       4.500us             2  \n",
      "        aten::_local_scalar_dense         0.00%       2.000us         0.00%       2.000us       1.000us             2  \n",
      "                quantized::linear        94.52%     142.649ms        94.84%     143.131ms     143.131ms             1  \n",
      "    aten::_empty_affine_quantized         0.01%      11.000us         0.01%      11.000us      11.000us             1  \n",
      "                    aten::q_scale         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "               aten::q_zero_point         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                    aten::resize_         0.31%     464.000us         0.31%     464.000us     464.000us             1  \n",
      "                      aten::empty         0.02%      36.000us         0.02%      36.000us      18.000us             2  \n",
      "                 aten::dequantize         2.71%       4.088ms         2.73%       4.118ms       4.118ms             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 150.925ms\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input):\n",
      "    input_1 = input\n",
      "    _0_input_scale_0 = getattr(self, \"0_input_scale_0\")\n",
      "    _0_input_zero_point_0 = getattr(self, \"0_input_zero_point_0\")\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(input_1, _0_input_scale_0, _0_input_zero_point_0, torch.quint8);  input_1 = _0_input_scale_0 = _0_input_zero_point_0 = None\n",
      "    _0 = getattr(self, \"0\")(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    _1 = getattr(self, \"1\")(_0);  _0 = None\n",
      "    dequantize_2 = _1.dequantize();  _1 = None\n",
      "    return dequantize_2\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-12-10 17:08:27 55896:55896 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-12-10 17:08:27 55896:55896 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-12-10 17:08:27 55896:55896 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "\n",
    "with torch.inference_mode():\n",
    "    torch.backends.quantized.engine = 'x86'\n",
    "    qconfig = get_default_qconfig('x86')\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    example_inputs = torch.randn_like(x)\n",
    "    prepared_model = prepare_fx(linear, qconfig_mapping, example_inputs)\n",
    "    for _ in range(16):\n",
    "        prepared_model(torch.randn_like(x))\n",
    "\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        quantized_model(x)\n",
    "\n",
    "print(prof.key_averages().table())\n",
    "print(quantized_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.jit.save(jit, f'x86_linear_{n}x{m}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  _input_scale_0 = self._input_scale_0\n",
      "  _input_zero_point_0 = self._input_zero_point_0\n",
      "  quantize_per_tensor = torch.quantize_per_tensor(input, _input_scale_0, _input_zero_point_0, 13)\n",
      "  _packed_weight_0 = self._packed_weight_0\n",
      "  _scale_1 = self._scale_1\n",
      "  _zero_point_1 = self._zero_point_1\n",
      "  linear = ops.quantized.linear(quantize_per_tensor, _packed_weight_0, annotate(float, _scale_1), annotate(int, _zero_point_1))\n",
      "  return torch.dequantize(linear)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/.local/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jit = torch.jit.script(quantized_model)\n",
    "print(jit.code)\n",
    "torch.jit.save(jit, f'x86_linear_{n}x{m}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::quantize_per_tensor         2.47%       2.741ms         2.48%       2.751ms       2.751ms             1  \n",
      "                       aten::item         0.02%      18.000us         0.02%      24.000us       6.000us             4  \n",
      "        aten::_local_scalar_dense         0.01%       7.000us         0.01%       7.000us       1.750us             4  \n",
      "                quantized::linear        95.29%     105.915ms        95.32%     105.951ms     105.951ms             1  \n",
      "    aten::_empty_affine_quantized         0.01%      16.000us         0.01%      16.000us      16.000us             1  \n",
      "                    aten::q_scale         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "               aten::q_zero_point         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                    aten::resize_         0.01%      14.000us         0.01%      14.000us      14.000us             1  \n",
      "                      aten::empty         0.02%      27.000us         0.02%      27.000us      13.500us             2  \n",
      "                 aten::dequantize         2.17%       2.410ms         2.19%       2.432ms       2.432ms             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 111.149ms\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input : torch.Tensor) -> torch.Tensor:\n",
      "    input_1 = input\n",
      "    _input_scale_0 = self._input_scale_0\n",
      "    _input_zero_point_0 = self._input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(input_1, _input_scale_0, _input_zero_point_0, torch.quint8);  input_1 = _input_scale_0 = _input_zero_point_0 = None\n",
      "    _packed_weight_0 = self._packed_weight_0\n",
      "    _scale_1 = self._scale_1\n",
      "    _zero_point_1 = self._zero_point_1\n",
      "    linear = torch.ops.quantized.linear(quantize_per_tensor, _packed_weight_0, _scale_1, _zero_point_1);  quantize_per_tensor = _packed_weight_0 = _scale_1 = _zero_point_1 = None\n",
      "    dequantize_2 = linear.dequantize();  linear = None\n",
      "    return dequantize_2\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/.local/lib/python3.10/site-packages/torch/fx/graph.py:1377: UserWarning: Node _packed_weight_0 target _packed_weight_0 _packed_weight_0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "STAGE:2023-12-10 16:55:33 55896:55896 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-12-10 16:55:34 55896:55896 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-12-10 16:55:34 55896:55896 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "\n",
    "with torch.inference_mode():\n",
    "    torch.backends.quantized.engine = 'fbgemm'\n",
    "    qconfig = get_default_qconfig('fbgemm')\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    example_inputs = torch.randn_like(x)\n",
    "    prepared_model = prepare_fx(linear, qconfig_mapping, example_inputs)\n",
    "    for _ in range(16):\n",
    "        prepared_model(torch.randn_like(x))\n",
    "\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        quantized_model(x)\n",
    "\n",
    "print(prof.key_averages().table())\n",
    "print(quantized_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  _input_scale_0 = self._input_scale_0\n",
      "  _input_zero_point_0 = self._input_zero_point_0\n",
      "  quantize_per_tensor = torch.quantize_per_tensor(input, _input_scale_0, _input_zero_point_0, 13)\n",
      "  _packed_weight_0 = self._packed_weight_0\n",
      "  _scale_1 = self._scale_1\n",
      "  _zero_point_1 = self._zero_point_1\n",
      "  linear = ops.quantized.linear(quantize_per_tensor, _packed_weight_0, annotate(float, _scale_1), annotate(int, _zero_point_1))\n",
      "  return torch.dequantize(linear)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/.local/lib/python3.10/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jit = torch.jit.script(quantized_model)\n",
    "print(jit.code)\n",
    "torch.jit.save(jit, f'fbgemm_linear_{n}x{m}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self : __torch__.torch.fx.graph_module.GraphModule,\n",
      "      %input.1 : Tensor):\n",
      "  %8 : int = prim::Constant[value=13]() # <eval_with_key>.15:8:98\n",
      "  %_input_scale_0.1 : Tensor = prim::GetAttr[name=\"_input_scale_0\"](%self)\n",
      "  %_input_zero_point_0.1 : Tensor = prim::GetAttr[name=\"_input_zero_point_0\"](%self)\n",
      "  %quantize_per_tensor.1 : Tensor = aten::quantize_per_tensor(%input.1, %_input_scale_0.1, %_input_zero_point_0.1, %8) # <eval_with_key>.15:8:26\n",
      "  %_packed_weight_0.1 : __torch__.torch.classes.quantized.LinearPackedParamsBase = prim::GetAttr[name=\"_packed_weight_0\"](%self)\n",
      "  %_scale_1.1 : Tensor = prim::GetAttr[name=\"_scale_1\"](%self)\n",
      "  %_zero_point_1.1 : Tensor = prim::GetAttr[name=\"_zero_point_1\"](%self)\n",
      "  %21 : float = aten::FloatImplicit(%_scale_1.1) # <eval_with_key>.15:12:13\n",
      "  %22 : int = aten::IntImplicit(%_zero_point_1.1) # <eval_with_key>.15:12:13\n",
      "  %linear.1 : Tensor = quantized::linear(%quantize_per_tensor.1, %_packed_weight_0.1, %21, %22) # <eval_with_key>.15:12:13\n",
      "  %dequantize_2.1 : Tensor = aten::dequantize(%linear.1) # <eval_with_key>.15:13:19\n",
      "  return (%dequantize_2.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(jit.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::quantize_per_tensor         0.23%      60.000us         0.26%      68.000us      68.000us             1  \n",
      "                       aten::item         0.03%       9.000us         0.04%      11.000us       2.750us             4  \n",
      "        aten::_local_scalar_dense         0.01%       2.000us         0.01%       2.000us       0.500us             4  \n",
      "                quantized::conv2d        35.61%       9.285ms        36.00%       9.387ms       9.387ms             1  \n",
      "                 aten::contiguous         0.01%       2.000us         0.36%      95.000us      95.000us             1  \n",
      "                      aten::clone         0.33%      87.000us         0.36%      93.000us      93.000us             1  \n",
      "                    aten::qscheme         0.00%       0.000us         0.00%       0.000us       0.000us             3  \n",
      "               aten::q_zero_point         0.00%       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                    aten::q_scale         0.00%       0.000us         0.00%       0.000us       0.000us             2  \n",
      "    aten::_empty_affine_quantized         0.03%       8.000us         0.03%       8.000us       4.000us             2  \n",
      "                      aten::empty         0.06%      16.000us         0.06%      16.000us       8.000us             2  \n",
      "                 aten::dequantize        63.68%      16.603ms        63.72%      16.614ms      16.614ms             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 26.072ms\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input : torch.Tensor) -> torch.Tensor:\n",
      "    input_1 = input\n",
      "    _input_scale_0 = self._input_scale_0\n",
      "    _input_zero_point_0 = self._input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(input_1, _input_scale_0, _input_zero_point_0, torch.quint8);  input_1 = _input_scale_0 = _input_zero_point_0 = None\n",
      "    _packed_weight_0 = self._packed_weight_0\n",
      "    _scale_1 = self._scale_1\n",
      "    _zero_point_1 = self._zero_point_1\n",
      "    conv2d = torch.ops.quantized.conv2d(quantize_per_tensor, _packed_weight_0, _scale_1, _zero_point_1);  quantize_per_tensor = _packed_weight_0 = _scale_1 = _zero_point_1 = None\n",
      "    dequantize_2 = conv2d.dequantize();  conv2d = None\n",
      "    return dequantize_2\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-11-20 19:58:43 112085:112085 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-20 19:58:43 112085:112085 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-20 19:58:43 112085:112085 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "\n",
    "conv2d = nn.Conv2d(16, 8, 3)\n",
    "image = torch.randn(1, 16, 64, 64)\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    qconfig = get_default_qconfig('fbgemm')\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    example_inputs = torch.randn_like(image)\n",
    "    prepared_model = prepare_fx(conv2d, qconfig_mapping, example_inputs)\n",
    "    for _ in range(16):\n",
    "        prepared_model(torch.randn_like(image))\n",
    "\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        quantized_model(image)\n",
    "\n",
    "print(prof.key_averages().table())\n",
    "print(quantized_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
