{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch.multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "transforms = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../imagenet1000.txt', 'r') as fopen:\n",
    "    lines = fopen.readlines()\n",
    "\n",
    "def process_classes(line: str):\n",
    "    splitted = line.strip().removeprefix('{').removesuffix(',').split(':')\n",
    "    return (int(splitted[0]), splitted[1].strip().strip('\\''))\n",
    "\n",
    "orig_classes = dict(map(process_classes, lines))\n",
    "\n",
    "imagenette_classes = dict(enumerate(['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']))\n",
    "\n",
    "for k, v in imagenette_classes.items():\n",
    "    for k1, v1 in orig_classes.items():\n",
    "        if v in v1:\n",
    "            imagenette_classes[k] = k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasource, transforms: callable, ramming: bool = False):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.ramming = ramming\n",
    "        if ramming:\n",
    "            ram_data = []\n",
    "            for i in range(len(datasource)):\n",
    "                data = datasource[i]\n",
    "                ram_data.append({'image': data['image'], 'label': data['label']})\n",
    "            self.datasource = ram_data\n",
    "        else:\n",
    "            self.datasource = datasource\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datasource)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        data = self.datasource[index]\n",
    "        image, label = data['image'], data['label']\n",
    "        if image.mode != 'RGB':\n",
    "            image = Image.fromarray(np.array(image)[..., None].repeat(3, -1))\n",
    "        return self.transforms(image), imagenette_classes[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_train = load_dataset('frgfm/imagenette', '320px', split='train')\n",
    "imagenette_valid = load_dataset('frgfm/imagenette', '320px', split='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiny_imagenet_train = load_dataset('Maysee/tiny-imagenet', split='train')\n",
    "tiny_imagenet_valid = load_dataset('Maysee/tiny-imagenet', split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = Dataset(datasource=tiny_imagenet_train, transforms=transforms())\n",
    "tf = transforms()\n",
    "trainset = Dataset(datasource=imagenette_train, transforms=tf)\n",
    "validset = Dataset(datasource=imagenette_valid, transforms=tf, ramming=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, batch_size=batch_size, shuffle=False)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbytes(model: torch.nn.Module):\n",
    "    n = 0\n",
    "    for p in model.parameters():\n",
    "        n += p.nbytes\n",
    "\n",
    "    return n / 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330.2294006347656"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbytes(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efcf8b941b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from itertools import product\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "from timeit import timeit\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "\n",
    "def fix_seed(worker_id=0, seed=0xBADCAFE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "torch_generator = torch.Generator()\n",
    "torch_generator.manual_seed(0xBADCAFFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_inputs = [trainset[i][0] for i in map(int, np.unique(np.random.randint(0, len(trainset), size=128)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_precision = ['medium', 'high', 'highest']\n",
    "quantization = ['None', 'x86', 'fbgemm']\n",
    "mixed_precision = ['']\n",
    "batch_sizes = [1, 4]\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, valid_dataloader, limit=2**32):\n",
    "    T = 0.0\n",
    "    Y = []\n",
    "    Y_hat = []\n",
    "    for i, (x, y) in enumerate(valid_dataloader):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        Y.append(y.ravel())\n",
    "        start = time.time()\n",
    "        y_hat = model(x)\n",
    "        end = time.time()\n",
    "        Y_hat.append(y_hat.argmax(-1))\n",
    "        T += end - start\n",
    "    return accuracy_score(np.array(Y).ravel(), np.array(Y_hat).ravel()), T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([574])\n",
      "tensor([569])\n",
      "tensor([497])\n",
      "tensor([482])\n",
      "tensor([491])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([571])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([566])\n",
      "tensor([574])\n",
      "tensor([482])\n",
      "tensor([701])\n",
      "tensor([571])\n",
      "tensor([0])\n",
      "tensor([491])\n",
      "tensor([482])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:08,  4.32s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([566])\n",
      "tensor([497])\n",
      "tensor([569])\n",
      "tensor([571])\n",
      "tensor([574])\n",
      "tensor([566])\n",
      "tensor([701])\n",
      "tensor([217])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:14,  5.11s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([217])\n",
      "tensor([482])\n",
      "tensor([701])\n",
      "tensor([566])\n",
      "tensor([566])\n",
      "tensor([574])\n",
      "tensor([0])\n",
      "tensor([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:20,  5.53s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([566])\n",
      "tensor([574])\n",
      "tensor([569])\n",
      "tensor([574])\n",
      "tensor([571])\n",
      "tensor([569])\n",
      "tensor([497])\n",
      "tensor([566])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:27,  5.90s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([574])\n",
      "tensor([571])\n",
      "tensor([491])\n",
      "tensor([491])\n",
      "tensor([0])\n",
      "tensor([217])\n",
      "tensor([491])\n",
      "tensor([701])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:33,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([491])\n",
      "tensor([571])\n",
      "tensor([569])\n",
      "tensor([566])\n",
      "tensor([482])\n",
      "tensor([217])\n",
      "tensor([217])\n",
      "tensor([566])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:37,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([701])\n",
      "tensor([217])\n",
      "tensor([571])\n",
      "tensor([491])\n",
      "tensor([217])\n",
      "tensor([497])\n",
      "tensor([497])\n",
      "tensor([497])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:41,  4.76s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([497])\n",
      "tensor([569])\n",
      "tensor([566])\n",
      "tensor([566])\n",
      "tensor([566])\n",
      "tensor([0])\n",
      "tensor([482])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:47,  5.21s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([574])\n",
      "tensor([569])\n",
      "tensor([497])\n",
      "tensor([217])\n",
      "tensor([566])\n",
      "tensor([482])\n",
      "tensor([569])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:53,  5.37s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([574])\n",
      "tensor([569])\n",
      "tensor([482])\n",
      "tensor([217])\n",
      "tensor([571])\n",
      "tensor([497])\n",
      "tensor([482])\n",
      "tensor([701])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:59,  5.58s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([217])\n",
      "tensor([491])\n",
      "tensor([569])\n",
      "tensor([701])\n",
      "tensor([491])\n",
      "tensor([0])\n",
      "tensor([566])\n",
      "tensor([497])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:05,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([217])\n",
      "tensor([497])\n",
      "tensor([217])\n",
      "tensor([701])\n",
      "tensor([566])\n",
      "tensor([571])\n",
      "tensor([491])\n",
      "tensor([701])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:09,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([497])\n",
      "tensor([491])\n",
      "tensor([574])\n",
      "tensor([569])\n",
      "tensor([217])\n",
      "tensor([497])\n",
      "tensor([217])\n",
      "tensor([574])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:13,  4.85s/it]/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "14it [01:16,  5.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     qconfig_mapping \u001b[39m=\u001b[39m get_default_qconfig_mapping(quant)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     prepared_model \u001b[39m=\u001b[39m prepare_fx(model, qconfig_mapping, example_inputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model \u001b[39m=\u001b[39m convert_fx(prepared_model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m key  \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, [prec, quant, bs, \u001b[39mround\u001b[39m(nbytes(model))]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:596\u001b[0m, in \u001b[0;36mconvert_fx\u001b[0;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\" Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 596\u001b[0m \u001b[39mreturn\u001b[39;00m _convert_fx(\n\u001b[1;32m    597\u001b[0m     graph_module,\n\u001b[1;32m    598\u001b[0m     is_reference\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    599\u001b[0m     convert_custom_config\u001b[39m=\u001b[39;49mconvert_custom_config,\n\u001b[1;32m    600\u001b[0m     _remove_qconfig\u001b[39m=\u001b[39;49m_remove_qconfig,\n\u001b[1;32m    601\u001b[0m     qconfig_mapping\u001b[39m=\u001b[39;49mqconfig_mapping,\n\u001b[1;32m    602\u001b[0m     backend_config\u001b[39m=\u001b[39;49mbackend_config,\n\u001b[1;32m    603\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:524\u001b[0m, in \u001b[0;36m_convert_fx\u001b[0;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed)\u001b[0m\n\u001b[1;32m    521\u001b[0m preserved_attr_names \u001b[39m=\u001b[39m convert_custom_config\u001b[39m.\u001b[39mpreserved_attributes\n\u001b[1;32m    522\u001b[0m preserved_attrs \u001b[39m=\u001b[39m {attr: \u001b[39mgetattr\u001b[39m(graph_module, attr) \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m preserved_attr_names \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(graph_module, attr)}\n\u001b[0;32m--> 524\u001b[0m quantized \u001b[39m=\u001b[39m convert(\n\u001b[1;32m    525\u001b[0m     graph_module,\n\u001b[1;32m    526\u001b[0m     is_reference,\n\u001b[1;32m    527\u001b[0m     convert_custom_config,\n\u001b[1;32m    528\u001b[0m     is_standalone_module,\n\u001b[1;32m    529\u001b[0m     _remove_qconfig_flag\u001b[39m=\u001b[39;49m_remove_qconfig,\n\u001b[1;32m    530\u001b[0m     qconfig_mapping\u001b[39m=\u001b[39;49mqconfig_mapping,\n\u001b[1;32m    531\u001b[0m     backend_config\u001b[39m=\u001b[39;49mbackend_config,\n\u001b[1;32m    532\u001b[0m     is_decomposed\u001b[39m=\u001b[39;49mis_decomposed,\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    535\u001b[0m attach_preserved_attrs_to_model(quantized, preserved_attrs)\n\u001b[1;32m    536\u001b[0m \u001b[39mreturn\u001b[39;00m quantized\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fx/convert.py:1091\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config, is_decomposed)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# TODO: maybe move this to quantize_fx.py\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_reference:\n\u001b[0;32m-> 1091\u001b[0m     model \u001b[39m=\u001b[39m lower_to_fbgemm(model, node_name_to_qconfig, node_name_to_scope)\n\u001b[1;32m   1093\u001b[0m \u001b[39m# TODO: this looks hacky, we want to check why we need this and see if we can\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[39m# remove this\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39m# removes qconfig and activation_post_process modules\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m _remove_qconfig_flag:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fx/lower_to_fbgemm.py:16\u001b[0m, in \u001b[0;36mlower_to_fbgemm\u001b[0;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlower_to_fbgemm\u001b[39m(\n\u001b[1;32m      9\u001b[0m     model: GraphModule,\n\u001b[1;32m     10\u001b[0m     qconfig_map: Dict[\u001b[39mstr\u001b[39m, QConfigAny],\n\u001b[1;32m     11\u001b[0m     node_name_to_scope: Dict[\u001b[39mstr\u001b[39m, Tuple[\u001b[39mstr\u001b[39m, \u001b[39mtype\u001b[39m]]\n\u001b[1;32m     12\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\"\" Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m    to fbgemm\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mreturn\u001b[39;00m _lower_to_native_backend(model, qconfig_map, node_name_to_scope)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:1153\u001b[0m, in \u001b[0;36m_lower_to_native_backend\u001b[0;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lower_to_native_backend\u001b[39m(\n\u001b[1;32m   1145\u001b[0m     model: GraphModule,\n\u001b[1;32m   1146\u001b[0m     qconfig_map: Dict[\u001b[39mstr\u001b[39m, QConfigAny],\n\u001b[1;32m   1147\u001b[0m     node_name_to_scope: Dict[\u001b[39mstr\u001b[39m, Tuple[\u001b[39mstr\u001b[39m, \u001b[39mtype\u001b[39m]]\n\u001b[1;32m   1148\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[1;32m   1149\u001b[0m     \u001b[39m\"\"\" Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[39m    to the native backend in PyTorch (fbgemm/qnnpack), both backends shares the same\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[39m    operator signature so they can be lowered with the same function\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1153\u001b[0m     _lower_static_weighted_ref_module(model, qconfig_map)\n\u001b[1;32m   1154\u001b[0m     _lower_static_weighted_ref_module_with_two_inputs(model, qconfig_map)\n\u001b[1;32m   1155\u001b[0m     _lower_dynamic_weighted_ref_module(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:619\u001b[0m, in \u001b[0;36m_lower_static_weighted_ref_module\u001b[0;34m(model, qconfig_map)\u001b[0m\n\u001b[1;32m    617\u001b[0m output_scale \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(model, scale_node\u001b[39m.\u001b[39mtarget)\n\u001b[1;32m    618\u001b[0m output_zero_point \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(model, zero_point_node\u001b[39m.\u001b[39mtarget)\n\u001b[0;32m--> 619\u001b[0m q_module \u001b[39m=\u001b[39m q_class\u001b[39m.\u001b[39;49mfrom_reference(ref_module, output_scale, output_zero_point)\n\u001b[1;32m    620\u001b[0m \u001b[39m# replace reference module with quantized module\u001b[39;00m\n\u001b[1;32m    621\u001b[0m parent_name, module_name \u001b[39m=\u001b[39m _parent_name(ref_node\u001b[39m.\u001b[39mtarget)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:299\u001b[0m, in \u001b[0;36mLinear.from_reference\u001b[0;34m(cls, ref_qlinear, output_scale, output_zero_point)\u001b[0m\n\u001b[1;32m    295\u001b[0m qlinear \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    296\u001b[0m     ref_qlinear\u001b[39m.\u001b[39min_features,\n\u001b[1;32m    297\u001b[0m     ref_qlinear\u001b[39m.\u001b[39mout_features)\n\u001b[1;32m    298\u001b[0m qweight \u001b[39m=\u001b[39m ref_qlinear\u001b[39m.\u001b[39mget_quantized_weight()\n\u001b[0;32m--> 299\u001b[0m qlinear\u001b[39m.\u001b[39;49mset_weight_bias(qweight, ref_qlinear\u001b[39m.\u001b[39;49mbias)\n\u001b[1;32m    301\u001b[0m qlinear\u001b[39m.\u001b[39mscale \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(output_scale)\n\u001b[1;32m    302\u001b[0m qlinear\u001b[39m.\u001b[39mzero_point \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(output_zero_point)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:241\u001b[0m, in \u001b[0;36mLinear.set_weight_bias\u001b[0;34m(self, w, b)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_weight_bias\u001b[39m(\u001b[39mself\u001b[39m, w: torch\u001b[39m.\u001b[39mTensor, b: Optional[torch\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_packed_params\u001b[39m.\u001b[39;49mset_weight_bias(w, b)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:32\u001b[0m, in \u001b[0;36mLinearPackedParams.set_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mexport\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_weight_bias\u001b[39m(\u001b[39mself\u001b[39m, weight: torch\u001b[39m.\u001b[39mTensor, bias: Optional[torch\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mqint8:\n\u001b[0;32m---> 32\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_packed_params \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mquantized\u001b[39m.\u001b[39;49mlinear_prepack(weight, bias)\n\u001b[1;32m     33\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_packed_params \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mquantized\u001b[39m.\u001b[39mlinear_prepack_fp16(weight, bias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "limit = 16\n",
    "T = {}\n",
    "accuracy = {}\n",
    "with torch.no_grad():\n",
    "    for prec, quant, bs in tqdm(product(matmul_precision, quantization, batch_sizes)):\n",
    "        valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, \n",
    "                                                       batch_size=batch_size, shuffle=True, \n",
    "                                                       worker_init_fn=fix_seed, generator=torch_generator)\n",
    "        model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1).eval()\n",
    "        torch.set_float32_matmul_precision(prec)\n",
    "        if quant != 'None':\n",
    "            torch.backends.quantized.engine = quant\n",
    "            qconfig_mapping = get_default_qconfig_mapping(quant)\n",
    "            prepared_model = prepare_fx(model, qconfig_mapping, example_inputs=examples_inputs)\n",
    "            model = convert_fx(prepared_model)\n",
    "        key  = '_'.join(map(str, [prec, quant, bs, round(nbytes(model))]))\n",
    "        acc, t = run_epoch(model, valid_dataloader, limit)\n",
    "        \n",
    "        T[key] = np.round(t / (min(limit, len(valid_dataloader)) * bs), 3)\n",
    "        accuracy[key] = np.round(acc, 3)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium_None_1_330 time: 0.334 acc 0.875\n",
      "medium_None_4_330 time: 0.087 acc 1.0\n",
      "medium_x86_1_109 time: 0.35 acc 0.25\n",
      "medium_x86_4_109 time: 0.063 acc 0.0\n",
      "medium_fbgemm_1_109 time: 0.244 acc 0.0\n",
      "medium_fbgemm_4_109 time: 0.066 acc 0.0\n",
      "high_None_1_330 time: 0.275 acc 1.0\n",
      "high_None_4_330 time: 0.098 acc 1.0\n",
      "high_x86_1_109 time: 0.27 acc 0.0\n",
      "high_x86_4_109 time: 0.069 acc 0.0\n",
      "high_fbgemm_1_109 time: 0.218 acc 0.0\n",
      "high_fbgemm_4_109 time: 0.066 acc 0.125\n",
      "highest_None_1_330 time: 0.289 acc 1.0\n",
      "highest_None_4_330 time: 0.066 acc 1.0\n",
      "highest_x86_1_109 time: 0.207 acc 0.125\n",
      "highest_x86_4_109 time: 0.054 acc 0.0\n",
      "highest_fbgemm_1_109 time: 0.236 acc 0.375\n",
      "highest_fbgemm_4_109 time: 0.059 acc 0.125\n"
     ]
    }
   ],
   "source": [
    "for key, t in T.items():\n",
    "    print(key, 'time:', t, 'acc', accuracy[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_time = datetime.datetime.now()\n",
    "date_time\n",
    "with open(f'results{date_time}.txt', 'w+') as fopen:\n",
    "    for key, t in T.items():\n",
    "        fopen.write(f\"{key}, time:, {t}, acc, {accuracy[key]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
