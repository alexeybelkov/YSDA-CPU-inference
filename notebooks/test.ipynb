{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch.multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "transforms = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../imagenet1000.txt', 'r') as fopen:\n",
    "    lines = fopen.readlines()\n",
    "\n",
    "def process_classes(line: str):\n",
    "    splitted = line.strip().removeprefix('{').removesuffix(',').split(':')\n",
    "    return (int(splitted[0]), splitted[1].strip().strip('\\''))\n",
    "\n",
    "orig_classes = dict(map(process_classes, lines))\n",
    "\n",
    "imagenette_classes = dict(enumerate(['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']))\n",
    "\n",
    "for k, v in imagenette_classes.items():\n",
    "    for k1, v1 in orig_classes.items():\n",
    "        if v in v1:\n",
    "            imagenette_classes[k] = k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasource, transforms: callable):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.datasource = datasource\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datasource)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        data = self.datasource[index]\n",
    "        image, label = data['image'], data['label']\n",
    "        if image.mode != 'RGB':\n",
    "            image = Image.fromarray(np.array(image)[..., None].repeat(3, -1))\n",
    "        return self.transforms(image), imagenette_classes[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_train = load_dataset('frgfm/imagenette', '320px', split='train')\n",
    "imagenette_valid = load_dataset('frgfm/imagenette', '320px', split='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiny_imagenet_train = load_dataset('Maysee/tiny-imagenet', split='train')\n",
    "tiny_imagenet_valid = load_dataset('Maysee/tiny-imagenet', split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = Dataset(datasource=tiny_imagenet_train, transforms=transforms())\n",
    "tf = transforms()\n",
    "trainset = Dataset(datasource=imagenette_train, transforms=tf)\n",
    "validset = Dataset(datasource=imagenette_valid, transforms=tf)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, batch_size=batch_size, shuffle=False)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbytes(model: torch.nn.Module):\n",
    "    n = 0\n",
    "    for p in model.parameters():\n",
    "        n += p.nbytes\n",
    "\n",
    "    return n / 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 4\n",
      "\tat::get_num_interop_threads() : 4\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 4\n",
      "Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 4\n",
      "Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
      "std::thread::hardware_concurrency() : 8\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__config__.parallel_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330.2294006347656"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbytes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fda24163650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from itertools import product\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "from timeit import timeit\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "import datetime\n",
    "\n",
    "def fix_seed(worker_id=0, seed=0xBADCAFE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "torch_generator = torch.Generator()\n",
    "torch_generator.manual_seed(0xBADCAFFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization = ['x86', 'fbgemm']\n",
    "mixed_precision = ['']\n",
    "batch_sizes = [1, 4]\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, valid_dataloader, limit=2**32):\n",
    "    T = 0.0\n",
    "    Y = []\n",
    "    Y_hat = []\n",
    "    for i, (x, y) in enumerate(valid_dataloader):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        Y.append(y.ravel())\n",
    "        start = time.time()\n",
    "        y_hat = model(x)\n",
    "        end = time.time()\n",
    "        Y_hat.append(y_hat.argmax(-1))\n",
    "        T += end - start\n",
    "    return accuracy_score(np.array(Y).ravel(), np.array(Y_hat).ravel()), T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            if i > 64:\n",
    "                break\n",
    "            model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/home/alexey/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "STAGE:2023-10-18 02:48:21 189450:189450 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-10-18 02:48:24 189450:189450 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-10-18 02:48:24 189450:189450 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "0it [00:35, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             model(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m fopen\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexey/YSDA/YSDA-CPU-inference/notebooks/test.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m fopen\u001b[39m.\u001b[39mwrite(prof\u001b[39m.\u001b[39mkey_averages()\u001b[39m.\u001b[39mtable(sort_by\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu_time_total\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "limit = 16\n",
    "T = {}\n",
    "date_time = datetime.datetime.now()\n",
    "accuracy = {}\n",
    "with torch.inference_mode():\n",
    "    with open(f'profiling{date_time}.txt', 'w+') as fopen:\n",
    "        for quant, bs in tqdm(product(quantization, batch_sizes)):\n",
    "            valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, \n",
    "                                                        batch_size=batch_size, shuffle=True, \n",
    "                                                        worker_init_fn=fix_seed, generator=torch_generator)\n",
    "            model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1).eval()\n",
    "            if quant != 'None':\n",
    "                torch.backends.quantized.engine = quant\n",
    "                qconfig_mapping = get_default_qconfig_mapping(quant)\n",
    "                prepared_model = prepare_fx(model, qconfig_mapping, example_inputs=next(iter(valid_dataloader))[0])\n",
    "                calibrate(prepared_model, valid_dataloader)\n",
    "                model = convert_fx(prepared_model)\n",
    "            key  = '_'.join(map(str, [quant, bs, round(nbytes(model))]))\n",
    "\n",
    "            with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "                for i, (x, y) in enumerate(valid_dataloader):\n",
    "                    with record_function(\"model_inference\"):\n",
    "                        if i >= limit:\n",
    "                            break\n",
    "                        model(x)\n",
    "            assert False\n",
    "            fopen.write(f'{key}\\n')\n",
    "            fopen.write(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n",
    "\n",
    "            # acc, t = run_epoch(model, valid_dataloader, limit)\n",
    "            \n",
    "            # T[key] = np.round(t / (min(limit, len(valid_dataloader)) * bs), 3)\n",
    "            # accuracy[key] = np.round(acc, 3)\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv_proj): QuantizedConv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), scale=0.052607402205467224, zero_point=62)\n",
       "  (encoder): Module(\n",
       "    (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "    (layers): Module(\n",
       "      (encoder_layer_0): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.07862671464681625, zero_point=70, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.037298478186130524, zero_point=64, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.06767959147691727, zero_point=95, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.021646762266755104, zero_point=63, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.0892757847905159, zero_point=108, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.022770265117287636, zero_point=64, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.07492197304964066, zero_point=100, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.015980573371052742, zero_point=64, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.06917013227939606, zero_point=92, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.05045114457607269, zero_point=75, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.13305889070034027, zero_point=67, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.34257230162620544, zero_point=62, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.070600226521492, zero_point=90, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.019975321367383003, zero_point=62, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.07275007665157318, zero_point=87, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.023790419101715088, zero_point=60, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.07692320644855499, zero_point=89, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.02419145777821541, zero_point=66, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.0720728188753128, zero_point=77, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.03510694205760956, zero_point=85, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.08944598585367203, zero_point=62, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.07871188968420029, zero_point=60, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): Module(\n",
       "        (ln_1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "        (ln_2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Module(\n",
       "          (0): QuantizedLinear(in_features=768, out_features=3072, scale=0.06467300653457642, zero_point=93, qscheme=torch.per_channel_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): QuantizedDropout(p=0.0, inplace=False)\n",
       "          (3): QuantizedLinear(in_features=3072, out_features=768, scale=0.08060607314109802, zero_point=64, qscheme=torch.per_channel_affine)\n",
       "          (4): QuantizedDropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Module(\n",
       "    (head): QuantizedLinear(in_features=768, out_features=1000, scale=0.09977441281080246, zero_point=30, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, t in T.items():\n",
    "    print(key, 'time:', t, 'acc', accuracy[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_time = datetime.datetime.now()\n",
    "date_time\n",
    "with open(f'results{date_time}.txt', 'w+') as fopen:\n",
    "    for key, t in T.items():\n",
    "        fopen.write(f\"{key}, time:, {t}, acc, {accuracy[key]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
