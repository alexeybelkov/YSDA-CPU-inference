{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT: 4 4\n",
      "4 4\n",
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 4\n",
      "\tat::get_num_interop_threads() : 4\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 4\n",
      "Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 4\n",
      "Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
      "std::thread::hardware_concurrency() : 8\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch.multiprocessing\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "print('DEFAULT:', torch.get_num_threads(), torch.get_num_interop_threads())\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# os.environ['MKL_NUM_THREADS'] = '1'\n",
    "# torch.set_num_threads(1), torch.set_num_interop_threads(1)\n",
    "print(torch.get_num_threads(), torch.get_num_interop_threads())\n",
    "print(torch.__config__.parallel_info())\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "transforms = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../imagenet1000.txt', 'r') as fopen:\n",
    "    lines = fopen.readlines()\n",
    "\n",
    "def process_classes(line: str):\n",
    "    splitted = line.strip().removeprefix('{').removesuffix(',').split(':')\n",
    "    return (int(splitted[0]), splitted[1].strip().strip('\\''))\n",
    "\n",
    "orig_classes = dict(map(process_classes, lines))\n",
    "\n",
    "imagenette_classes = dict(enumerate(['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']))\n",
    "\n",
    "for k, v in imagenette_classes.items():\n",
    "    for k1, v1 in orig_classes.items():\n",
    "        if v in v1:\n",
    "            imagenette_classes[k] = k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasource, transforms: callable):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.datasource = datasource\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datasource)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        data = self.datasource[index]\n",
    "        image, label = data['image'], data['label']\n",
    "        if image.mode != 'RGB':\n",
    "            image = Image.fromarray(np.array(image)[..., None].repeat(3, -1))\n",
    "        return self.transforms(image), imagenette_classes[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_train = load_dataset('frgfm/imagenette', '320px', split='train')\n",
    "imagenette_valid = load_dataset('frgfm/imagenette', '320px', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms()\n",
    "trainset = Dataset(datasource=imagenette_train, transforms=tf)\n",
    "validset = Dataset(datasource=imagenette_valid, transforms=tf)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validset, num_workers=num_workers, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbytes(model: torch.nn.Module):\n",
    "    n = 0\n",
    "    for p in model.parameters():\n",
    "        n += p.nbytes\n",
    "    return n / 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f045682ff90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from itertools import product\n",
    "import torch.quantization\n",
    "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
    "from torch.ao.quantization import get_default_qconfig_mapping, get_default_qconfig\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "from timeit import timeit\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "import datetime\n",
    "# import torch.quantization._numeric_suite as ns\n",
    "import torch.quantization._numeric_suite_fx as ns\n",
    "\n",
    "def fix_seed(worker_id=0, seed=0xBADCAFE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed()\n",
    "\n",
    "torch_generator = torch.Generator()\n",
    "torch_generator.manual_seed(0xBADCAFFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_model = deepcopy(model)\n",
    "module_a = deepcopy(model)\n",
    "module_a.heads.head = nn.Identity()\n",
    "module_b = model.heads.head\n",
    "# model = LoggerModule(module_a, module_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    quantized_module_a = torch.quantization.quantize_dynamic(module_a, {nn.Linear}, dtype=torch.qint8)\n",
    "    quantized_module_b = torch.quantization.quantize_dynamic(module_b, {nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_module_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.1201171875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbytes(quantized_module_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ceaab1f9a1e4ff4b3dbc3913fd99ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "gt = []\n",
    "pred = []\n",
    "# embeddings = []\n",
    "Y = []\n",
    "with torch.inference_mode():\n",
    "    for x, y in tqdm(valid_dataloader):\n",
    "        emb = quantized_module_a.forward(x)\n",
    "        # embeddings.append(emb)\n",
    "        y_hat = quantized_module_b(emb)\n",
    "        # Y.append(y_hat)\n",
    "        gt.append(y)\n",
    "        pred.append(y_hat.argmax(-1))\n",
    "    gt = torch.cat(gt).ravel().numpy()\n",
    "    pred = torch.cat(pred).ravel().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330.2294006347656, 111.1201171875)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbytes(model), nbytes(quantized_module_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8901910828025478"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
