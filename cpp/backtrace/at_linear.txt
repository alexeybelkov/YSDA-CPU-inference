#0  at::native::cpublas::gemm (transa=at::native::TransposeType::Transpose, transb=at::native::TransposeType::NoTranspose, m=64, n=128, k=256, alpha=1, a=0x555557d4d540, lda=256, b=0x555557d5d5c0, 
    ldb=256, beta=0, c=0x555557d7d640, ldc=64) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/CPUBlas.cpp:169
#1  0x00007fffe40c47ba in operator() (__closure=0x7fffffffc0f0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1513
#2  0x00007fffe40c5880 in operator() (__closure=0x7fffffffc310) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1513
#3  0x00007fffe40c6f27 in at::native::addmm_impl_cpu_ (result=..., self=..., m1=..., m2=..., beta=..., alpha=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1513
#4  0x00007fffe40c80d1 in at::native::structured_mm_out_cpu::impl (this=0x7fffffffc4c0, self=..., mat2=..., result=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1617
#5  0x00007fffe5b0cdd6 in at::(anonymous namespace)::wrapper_CPU_mm (self=..., mat2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/RegisterCPU.cpp:8643
#6  0x00007fffe5ceb6b6 in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&), at::(anonymous namespace)::wrapper_CPU_mm>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&> >::operator() (args#1=..., args#0=..., this=0x55555604ba50)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13
#7  c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&), at::(anonymous namespace)::wrapper_CPU_mm>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&> >, at::Tensor(const at::Tensor&, const at::Tensor&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &) (functor=0x55555604ba50, args#0=..., args#1=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468
#8  0x00007fffe4dcf95a in c10::callUnboxedKernelFunction<at::Tensor, at::Tensor const&, at::Tensor const&> (
    unboxed_kernel_func=0x7fffe5ceb61d <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&), at::(anonymous namespace)::wrapper_CPU_mm>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&> >, at::Tensor(const at::Tensor&, const at::Tensor&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &)>, functor=0x55555604ba50, dispatchKeySet=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50
#9  0x00007fffe4c7155f in c10::KernelFunction::call<at::Tensor, at::Tensor const&, at::Tensor const&> (dispatchKeySet=..., opHandle=..., this=0x5555555a6d58)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:103
#10 c10::Dispatcher::redispatch<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&) const (this=0x7ffff7bd8040 <c10::Dispatcher::realSingleton()::_singleton>, op=..., currentDispatchKeySet=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:690
#11 0x00007fffe561205e in c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)>::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&) const (args#1=..., 
    args#0=..., currentDispatchKeySet=..., this=<optimized out>) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:526
#12 at::_ops::mm::redispatch (dispatchKeySet=..., self=..., mat2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/Operators_3.cpp:3896
#13 0x00007fffe86c7c17 in at::redispatch::mm (dispatchKeySet=..., self=..., mat2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/RedispatchFunctions.h:5112
#14 0x00007fffe85dab1e in operator() (__closure=0x7fffffffc960) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12504
#15 0x00007fffe85db28d in torch::autograd::VariableType::(anonymous namespace)::mm (ks=..., self=..., mat2=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12505
#16 0x00007fffe86913ee in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&), torch::autograd::VariableType::(anonymous namespace)::mm>, at::Tensor, c10::guts::typelist::typelist<c10::DispatchKeySet, const at::Tensor&, const at::Tensor&> >::operator() (args#2=..., args#1=..., args#0=..., 
    this=0x5555574f7360) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13
#17 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&), torch::autograd::VariableType::(anonymous namespace)::mm>, at::Tensor, c10::guts::typelist::typelist<c10::DispatchKeySet, const at::Tensor&, const at::Tensor&> >, at::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &) (functor=0x5555574f7360, dispatchKeySet=..., args#0=..., args#1=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:485
#18 0x00007fffe4dcf95a in c10::callUnboxedKernelFunction<at::Tensor, at::Tensor const&, at::Tensor const&> (
    unboxed_kernel_func=0x7fffe8691339 <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&), torch::autograd::VariableType::(anonymous namespace)::mm>, at::Tensor, c10::guts::typelist::typelist<c10::DispatchKeySet, const at::Tensor&, const at::Tensor&> >, at::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &)>, functor=0x5555574f7360, dispatchKeySet=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50
#19 0x00007fffe5611e16 in c10::KernelFunction::call<at::Tensor, at::Tensor const&, at::Tensor const&> (dispatchKeySet=..., opHandle=..., this=0x5555555a7738)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:103
#20 c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const (op=..., this=0x7ffff7bd8040 <c10::Dispatcher::realSingleton()::_singleton>) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:673
#21 c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)>::call(at::Tensor const&, at::Tensor const&) const (args#1=..., args#0=..., this=<optimized out>)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:521
#22 at::_ops::mm::call (self=..., mat2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/Operators_3.cpp:3889
#23 0x00007fffe40de0ea in at::Tensor::mm (this=0x7fffffffd5a8, mat2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/core/TensorBody.h:2991
#24 0x00007fffe40cbe51 in at::native::_matmul_impl (out=..., tensor1=..., tensor2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1996
#25 0x00007fffe40cd413 in at::native::matmul (tensor1=..., tensor2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:2144
#26 0x00007fffe61bc23c in at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__matmul (self=..., other=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp:2753
#27 0x00007fffe62c11dc in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&), at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__matmul>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&> >::operator() (args#1=..., args#0=..., this=0x555556901540)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13
#28 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&), at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__matmul>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&> >, at::Tensor(const at::Tensor&, const at::Tensor&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &) (functor=0x555556901540, args#0=..., args#1=...)
    at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468
#29 0x00007fffe4dcf95a in c10::callUnboxedKernelFunction<at::Tensor, at::Tensor const&, at::Tensor const&> (
    unboxed_kernel_func=0x7fffe62c1143 <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&), at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__matmul>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&> >, at::Tensor(const at::Tensor&, const at::Tensor&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &)>, functor=0x555556901540, dispatchKeySet=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50
#30 0x00007fffe5898682 in c10::KernelFunction::call<at::Tensor, at::Tensor const&, at::Tensor const&> (dispatchKeySet=..., opHandle=..., this=0x55555565b9e8) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:103
#31 c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const (op=..., this=0x7ffff7bd8040 <c10::Dispatcher::realSingleton()::_singleton>) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:673
#32 c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)>::call(at::Tensor const&, at::Tensor const&) const (args#1=..., args#0=..., this=<optimized out>) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:521
#33 at::_ops::matmul::call (self=..., other=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/Operators_4.cpp:3052
#34 0x00007fffe39bd781 in at::matmul (self=..., other=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/ops/matmul.h:27
#35 0x00007fffe40a1e0b in at::native::linear (input=..., weight=..., bias_opt=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/Linear.cpp:106
#36 0x00007fffe61bbba3 in at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear (input=..., weight=..., bias=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp:2620
#37 0x00007fffe62bf064 in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&), at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&> >::operator() (args#2=..., args#1=..., args#0=..., this=0x5555568ef340) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13
#38 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&), at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&> >, at::Tensor(const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &) (functor=0x5555568ef340, args#0=..., args#1=..., args#2=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468
#39 0x00007fffe4de1f5e in c10::callUnboxedKernelFunction<at::Tensor, at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&> (unboxed_kernel_func=0x7fffe62befa0 <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor(const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&), at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear>, at::Tensor, c10::guts::typelist::typelist<const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&> >, at::Tensor(const at::Tensor&, const at::Tensor&, const c10::optional<at::Tensor>&)>::call(c10::OperatorKernel *, c10::DispatchKeySet, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &)>, functor=0x5555568ef340, dispatchKeySet=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50
#40 0x00007fffe4afe192 in c10::KernelFunction::call<at::Tensor, at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&> (dispatchKeySet=..., opHandle=..., this=0x5555557484f8) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:103
#41 c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&)> const&, at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&) const (op=..., this=0x7ffff7bd8040 <c10::Dispatcher::realSingleton()::_singleton>) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:673
#42 c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&)>::call(at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&) const (args#2=..., args#1=..., args#0=..., this=<optimized out>) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:521
#43 at::_ops::linear::call (input=..., weight=..., bias=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-build/aten/src/ATen/Operators_0.cpp:3601
#44 0x000055555555aa8a in at::linear (input=..., weight=..., bias=...) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch-install/include/ATen/ops/linear.h:27
#45 0x00005555555578e0 in main () at /home/alexey/YSDA/YSDA-CPU-inference/cpp/src/pytorch.cpp:17
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
[Inferior 1 (process 26362) exited normally]
Breakpoint 4 at 0x7fffe7978b88: /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:58. (18 locations)
Starting program: /home/alexey/YSDA/YSDA-CPU-inference/cpp/build/pytorch-exec 
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
[Inferior 1 (process 28362) exited normally]
Breakpoint 5 at 0x7fffe7978b88: /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:58. (18 locations)
Breakpoint 6 at 0x7fffe3f4fd72: file /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/CPUBlas.cpp, line 169.
Starting program: /home/alexey/YSDA/YSDA-CPU-inference/cpp/build/pytorch-exec 
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

Breakpoint 6, at::native::cpublas::gemm (transa=at::native::TransposeType::Transpose, transb=at::native::TransposeType::NoTranspose, m=64, n=128, k=256, alpha=1, a=0x555557d4d540, lda=256, b=0x555557d5d5c0, ldb=256, beta=0, c=0x555557d7d640, ldc=64) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/CPUBlas.cpp:169
169	  internal::normalize_last_dims(transa, transb, m, n, k, &lda, &ldb, &ldc);
199	  gemm_stub(
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:156
156	  rT operator()(c10::DeviceType device_type, ArgTypes&&... args) {
157	    FnPtr call_ptr = get_call_ptr(device_type);
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<long&> (__t=@0x7fffffffc030: 64) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<float*&> (__t=@0x7fffffffc028: 0x555557d7d640) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<float const&> (__t=@0x7fffffffbfd8: 0) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
c10::Scalar::Scalar (this=0x7fffffffbf10, vv=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:51
51	  AT_FORALL_SCALAR_TYPES_AND5(
c10::Scalar::Scalar<float, (bool*)0> (this=0x7fffffffbf10, vv=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:353
353	  Scalar(T vv, bool) : tag(Tag::HAS_d) {
c10::Scalar::v_t::v_t (this=0x7fffffffbf20) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:336
336	    v_t() {} // default constructor
c10::Scalar::Scalar<float, (bool*)0> (this=0x7fffffffbf10, vv=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:354
354	    v.d = convert<decltype(v.d), T>(vv);
c10::convert<double, float> (f=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:125
125	  return static_cast_with_inter_type<To, From>::apply(f);
c10::static_cast_with_inter_type<double, float>::apply (src=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:48
48	    constexpr bool real = needs_real<dest_t, src_t>::value;
49	    auto r = maybe_real<real, src_t>::apply(src);
c10::maybe_real<false, float>::apply (src=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:29
29	    return src;
30	  }
c10::static_cast_with_inter_type<double, float>::apply (src=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:50
50	    return static_cast<dest_t>(r);
51	  }
c10::convert<double, float> (f=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:126
126	}
c10::Scalar::Scalar<float, (bool*)0> (this=0x7fffffffbf10, vv=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:355
355	  }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<long&> (__t=@0x7fffffffc020: 256) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<float const*&> (__t=@0x7fffffffc018: 0x555557d5d5c0) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<long&> (__t=@0x7fffffffc010: 256) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<float const*&> (__t=@0x7fffffffbfd0: 0x555557d4d540) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
std::forward<float const&> (__t=@0x7fffffffbfdc: 1) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::DispatchStub<void (*)(c10::ScalarType, at::native::TransposeType, at::native::TransposeType, long, long, long, c10::Scalar const&, void const*, long, void const*, long, c10::Scalar const&, void*, long), at::native::cpublas::gemm_stub>::operator()<c10::ScalarType const&, at::native::TransposeType&, at::native::TransposeType&, long&, long&, long&, float const&, float const*&, long&, float const*&, long&, float const&, float*&, long&> (this=0x7ffff7bdcae0 <at::native::cpublas::gemm_stub>, device_type=c10::DeviceType::CPU) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/DispatchStub.h:158
158	    return (*call_ptr)(std::forward<ArgTypes>(args)...);
c10::Scalar::Scalar (this=0x7fffffffbef0, vv=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:51
51	  AT_FORALL_SCALAR_TYPES_AND5(
c10::Scalar::Scalar<float, (bool*)0> (this=0x7fffffffbef0, vv=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:353
353	  Scalar(T vv, bool) : tag(Tag::HAS_d) {
c10::Scalar::v_t::v_t (this=0x7fffffffbf00) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:336
336	    v_t() {} // default constructor
c10::Scalar::Scalar<float, (bool*)0> (this=0x7fffffffbef0, vv=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:354
354	    v.d = convert<decltype(v.d), T>(vv);
c10::convert<double, float> (f=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:125
125	  return static_cast_with_inter_type<To, From>::apply(f);
c10::static_cast_with_inter_type<double, float>::apply (src=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:48
48	    constexpr bool real = needs_real<dest_t, src_t>::value;
49	    auto r = maybe_real<real, src_t>::apply(src);
c10::maybe_real<false, float>::apply (src=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:29
29	    return src;
30	  }
c10::static_cast_with_inter_type<double, float>::apply (src=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:50
50	    return static_cast<dest_t>(r);
51	  }
c10::convert<double, float> (f=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:126
126	}
c10::Scalar::Scalar<float, (bool*)0> (this=0x7fffffffbef0, vv=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:355
355	  }
std::forward<long&> (__t=@0x7fffffffbfe0: 256) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
std::forward<long&> (__t=@0x7fffffffbfe8: 128) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
std::forward<long&> (__t=@0x7fffffffbff0: 64) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
std::forward<at::native::TransposeType&> (__t=@0x7fffffffbff8: at::native::TransposeType::NoTranspose) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
std::forward<at::native::TransposeType&> (__t=@0x7fffffffbffc: at::native::TransposeType::Transpose) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
std::forward<c10::ScalarType const&> (__t=@0x7ffff2972a70: c10::ScalarType::Float) at /usr/include/c++/11/bits/move.h:78
78	    { return static_cast<_Tp&&>(__t); }
at::native::cpublas::(anonymous namespace)::cpublas_gemm_impl (type=c10::ScalarType::Float, transa=at::native::TransposeType::Transpose, transb=at::native::TransposeType::NoTranspose, m=64, n=128, k=256, alpha=..., a=0x555557d4d540, lda=256, b=0x555557d5d5c0, ldb=256, beta=..., c=0x555557d7d640, ldc=64) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:285
285	    void *c, int64_t ldc) {
286	  _AT_DISPATCH_GEMM_TYPES(type, "cpublas_gemm_impl", [&]{
operator() (__closure=0x7fffffffbd50) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:286
286	  _AT_DISPATCH_GEMM_TYPES(type, "cpublas_gemm_impl", [&]{
detail::scalar_type (s=c10::ScalarType::Float) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/Dispatch.h:103
103	  return s;
104	}
operator() (__closure=0x7fffffffbc20) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:286
286	  _AT_DISPATCH_GEMM_TYPES(type, "cpublas_gemm_impl", [&]{
c10::Scalar::to<float> (this=0x7fffffffbf10) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:373
373	AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_TO)
c10::Scalar::toFloat (this=0x7fffffffbf10) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:103
103	  AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_ACCESSOR)
c10::checked_convert<float, double> (f=0, name=0x7ffff27ff819 "float") at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:134
134	  if (!std::is_same<To, bool>::value && overflows<To, From>(f)) {
c10::overflows<float, double> (f=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/Half.h:468
468	  if (limit::has_infinity && std::isinf(static_cast<double>(f))) {
std::isinf (__x=0) at /usr/include/c++/11/cmath:593
593	  { return __builtin_isinf(__x); }
c10::overflows<float, double> (f=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/Half.h:468
468	  if (limit::has_infinity && std::isinf(static_cast<double>(f))) {
474	  return f < limit::lowest() || f > limit::max();
std::numeric_limits<float>::lowest () at /usr/include/c++/11/limits:1680
1680	      lowest() noexcept { return -__FLT_MAX__; }
std::numeric_limits<float>::max () at /usr/include/c++/11/limits:1676
1676	      max() _GLIBCXX_USE_NOEXCEPT { return __FLT_MAX__; }
c10::overflows<float, double> (f=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/Half.h:475
475	}
c10::checked_convert<float, double> (f=0, name=0x7ffff27ff819 "float") at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:134
134	  if (!std::is_same<To, bool>::value && overflows<To, From>(f)) {
137	  return convert<To, From>(f);
c10::convert<float, double> (f=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:125
125	  return static_cast_with_inter_type<To, From>::apply(f);
c10::static_cast_with_inter_type<float, double>::apply (src=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:48
48	    constexpr bool real = needs_real<dest_t, src_t>::value;
49	    auto r = maybe_real<real, src_t>::apply(src);
c10::maybe_real<false, double>::apply (src=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:29
29	    return src;
30	  }
c10::static_cast_with_inter_type<float, double>::apply (src=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:50
50	    return static_cast<dest_t>(r);
51	  }
c10::convert<float, double> (f=0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:126
126	}
c10::checked_convert<float, double> (f=0, name=0x7ffff27ff819 "float") at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:138
138	}
c10::Scalar::to<float> (this=0x7fffffffbef0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:373
373	AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_TO)
c10::Scalar::toFloat (this=0x7fffffffbef0) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/core/Scalar.h:103
103	  AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_ACCESSOR)
c10::checked_convert<float, double> (f=1, name=0x7ffff27ff819 "float") at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:134
134	  if (!std::is_same<To, bool>::value && overflows<To, From>(f)) {
c10::overflows<float, double> (f=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/Half.h:468
468	  if (limit::has_infinity && std::isinf(static_cast<double>(f))) {
std::isinf (__x=1) at /usr/include/c++/11/cmath:593
593	  { return __builtin_isinf(__x); }
c10::overflows<float, double> (f=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/Half.h:468
468	  if (limit::has_infinity && std::isinf(static_cast<double>(f))) {
474	  return f < limit::lowest() || f > limit::max();
std::numeric_limits<float>::lowest () at /usr/include/c++/11/limits:1680
1680	      lowest() noexcept { return -__FLT_MAX__; }
std::numeric_limits<float>::max () at /usr/include/c++/11/limits:1676
1676	      max() _GLIBCXX_USE_NOEXCEPT { return __FLT_MAX__; }
c10::overflows<float, double> (f=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/Half.h:475
475	}
c10::checked_convert<float, double> (f=1, name=0x7ffff27ff819 "float") at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:134
134	  if (!std::is_same<To, bool>::value && overflows<To, From>(f)) {
137	  return convert<To, From>(f);
c10::convert<float, double> (f=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:125
125	  return static_cast_with_inter_type<To, From>::apply(f);
c10::static_cast_with_inter_type<float, double>::apply (src=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:48
48	    constexpr bool real = needs_real<dest_t, src_t>::value;
49	    auto r = maybe_real<real, src_t>::apply(src);
c10::maybe_real<false, double>::apply (src=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:29
29	    return src;
30	  }
c10::static_cast_with_inter_type<float, double>::apply (src=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:50
50	    return static_cast<dest_t>(r);
51	  }
c10::convert<float, double> (f=1) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:126
126	}
c10::checked_convert<float, double> (f=1, name=0x7ffff27ff819 "float") at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/c10/util/TypeCast.h:138
138	}
at::native::cpublas::(anonymous namespace)::gemm_core_<float, float> (transa=at::native::TransposeType::Transpose, transb=at::native::TransposeType::NoTranspose, m=64, n=128, k=256, alpha=1, a=0x555557d4d540, lda=256, b=0x555557d5d5c0, ldb=256, beta=0, c=0x555557d7d640, ldc=64) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:249
249	  if (transa == TransposeType::NoTranspose &&
252	  } else if (
253	      transa == TransposeType::Transpose &&
255	    gemm_transa_(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
at::native::cpublas::(anonymous namespace)::gemm_transa_<float, float> (m=64, n=128, k=256, alpha=1, a=0x555557d4d540, lda=256, b=0x555557d5d5c0, ldb=256, beta=0, c=0x555557d7d640, ldc=64) at /home/alexey/YSDA/YSDA-CPU-inference/cpp/pytorch/aten/src/ATen/native/cpu/BlasKernel.cpp:123
123	void gemm_transa_(
131	  const scalar_t *a_ = a;
132	  for (const auto i : c10::irange(m)) {
Quit
